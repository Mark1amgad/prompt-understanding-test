# -*- coding: utf-8 -*-
import os
import re
import gradio as gr
from huggingface_hub import InferenceClient
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# -------- Settings --------
# 1) Set your model here (open, non-gated is easiest). You can change it from the UI as well.
DEFAULT_MODEL = os.environ.get("MODEL_REPO", "HuggingFaceH4/zephyr-7b-beta")

# 2) Your HF token should be added as a Space secret named HF_TOKEN (Settings -> Secrets)
HF_TOKEN = os.environ.get("HF_TOKEN", None)

# Initialize clients/lite models lazily to speed up Space start
_embedder = None
def get_embedder():
    global _embedder
    if _embedder is None:
        # small, fast model (â‰ˆ60â€“90MB); suitable for Spaces CPU
        _embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    return _embedder

def generate_response(prompt: str, model_repo: str = DEFAULT_MODEL, max_new_tokens: int = 256, temperature: float = 0.7):
    if not prompt or not prompt.strip():
        return "", 0.0, "Ø§Ù„Ù€Prompt ÙØ§Ø±Øº."
    token = HF_TOKEN
    if token is None:
        return "", 0.0, "âš ï¸ Ù„Ù… ÙŠØªÙ… Ø¶Ø¨Ø· Ù…ÙØªØ§Ø­ HF_TOKEN ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù€Space."

    client = InferenceClient(model=model_repo, token=token)
    try:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© chat Ø¨Ø¯Ù„ text_generation
        chat_completion = client.chat.completions.create(
            model=model_repo,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_new_tokens,
            temperature=temperature,
        )
        text = chat_completion.choices[0].message["content"]
        return text, 1.0, "ØªÙ… Ø¨Ù†Ø¬Ø§Ø­."
    except Exception as e:
        return "", 0.0, f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}"

    if not prompt or not prompt.strip():
        return "", 0.0, "Ø§Ù„Ù€Prompt ÙØ§Ø±Øº."
    token = HF_TOKEN
    if token is None:
        return "", 0.0, "âš ï¸ Ù„Ù… ÙŠØªÙ… Ø¶Ø¨Ø· Ù…ÙØªØ§Ø­ HF_TOKEN ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù€Space (Settings â†’ Secrets)."

    client = InferenceClient(model=model_repo, token=token)
    try:
        # Basic text-generation call
        text = client.text_generation(
            prompt,
            max_new_tokens=int(max_new_tokens),
            temperature=float(temperature),
            top_p=0.95,
            repetition_penalty=1.05,
            do_sample=True,
            return_full_text=False,
        )
        return text, 1.0, "ØªÙ… Ø¨Ù†Ø¬Ø§Ø­."
    except Exception as e:
        return "", 0.0, f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}"

# ---------- Heuristic evaluation ----------
INSTRUCTION_HINTS = {
    "list_style": ["list", "Ù‚Ø§Ø¦Ù…Ø©", "Ø¹Ø¯Ù‘Ø¯", "Ø¹Ø¯", "bullet", "Ù†Ù‚Ø§Ø·", "â€¢", "â€“", "Ù¡.", "1."],
    "code_style": ["code", "ÙƒÙˆØ¯", "python", "Ø¬Ø§ÙØ§", "Ø³ÙŠ", "++c", "javascript", "js", "go", "rust"],
    "translate": ["ØªØ±Ø¬Ù…", "translate", "ØªØ±Ø¬Ù…Ø©", "to english", "Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", "to arabic"],
    "summarize": ["Ø®ØµÙ‘Øµ", "Ø§Ø®ØªØµØ±", "Ù„Ø®Ù‘Øµ", "summarize", "Ù…Ù„Ø®Øµ", "Ù…Ù„Ø®Ù‘Øµ"],
}

def detect_expected_format(prompt: str):
    p = prompt.lower()
    found = set()
    for k, kws in INSTRUCTION_HINTS.items():
        for kw in kws:
            if kw in p:
                found.add(k)
                break
    return found

def format_score(prompt: str, response: str):
    """Score if the response matches implied format from the prompt."""
    expected = detect_expected_format(prompt)
    if not expected:
        return 0.5, ["Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…Ø· Ù…Ø­Ø¯Ø¯ Ù…Ø·Ù„ÙˆØ¨ ÙÙŠ Ø§Ù„Ù€Prompt."]

    reasons = []
    score = 0.0

    if "list_style" in expected:
        # Check for bullet/numbered list
        has_bullets = bool(re.search(r"(^|\n)\s*(?:[-*â€¢â€“]|\d+\.)\s+\S", response))
        score += 1.0 if has_bullets else 0.0
        reasons.append("Ù‚Ø§Ø¦Ù…Ø©/Ù†Ù‚Ø§Ø·: " + ("âœ…" if has_bullets else "âŒ"))

    if "code_style" in expected:
        has_codeblock = "```" in response or bool(re.search(r"(^|\n)\s{4}\S", response))
        score += 1.0 if has_codeblock else 0.0
        reasons.append("ØªÙ†Ø³ÙŠÙ‚ ÙƒÙˆØ¯: " + ("âœ…" if has_codeblock else "âŒ"))

    if "translate" in expected:
        # naive: check if response contains non-Arabic when prompt Arabic says translate to English or vice-versa
        arabic_chars = re.findall(r"[\u0600-\u06FF]", response)
        latin_chars = re.findall(r"[A-Za-z]", response)
        has_mix = bool(arabic_chars) and bool(latin_chars)
        # If translation expected, a strong presence of the target alphabet is a weak signal.
        score += 0.8 if has_mix or len(latin_chars) > len(arabic_chars) else 0.0
        reasons.append("Ø¥Ø´Ø§Ø±Ø§Øª ØªØ±Ø¬Ù…Ø©: " + ("âœ…" if has_mix or len(latin_chars) > len(arabic_chars) else "âŒ"))

    if "summarize" in expected:
        # If summarize asked, shorter response than prompt (rough heuristic)
        score += 0.8 if len(response.split()) < max(40, len(prompt.split())) else 0.0
        reasons.append("ØªÙ„Ø®ÙŠØµ: " + ("âœ…" if len(response.split()) < max(40, len(prompt.split())) else "âŒ"))

    # Normalize to [0,1] by dividing by max possible (1 for list + 1 for code + 0.8 + 0.8 = 3.6)
    max_possible = 0.0
    max_possible += 1.0 if "list_style" in expected else 0.0
    max_possible += 1.0 if "code_style" in expected else 0.0
    max_possible += 0.8 if "translate" in expected else 0.0
    max_possible += 0.8 if "summarize" in expected else 0.0
    max_possible = max(max_possible, 1e-6)
    return float(score / max_possible), reasons

def similarity_score(prompt: str, response: str):
    emb = get_embedder()
    vecs = emb.encode([prompt, response])
    sim = cosine_similarity([vecs[0]], [vecs[1]])[0][0]
    return float(sim)  # -1..1

def length_score(response: str):
    # Reward informative responses; 0 at <=10 words; 1 at >= 60 words
    w = len(response.split())
    return float(np.clip((w - 10) / (60 - 10), 0, 1))

def overall_evaluate(prompt: str, response: str):
    """Return score 0..100, verdict, and explanations."""
    try:
        sim = similarity_score(prompt, response)  # -1..1
        # map -1..1 to 0..1
        sim01 = (sim + 1) / 2.0
    except Exception as e:
        sim01 = 0.0

    fmt, fmt_reasons = format_score(prompt, response)
    lng = length_score(response)

    # Weighted score
    score01 = 0.6 * sim01 + 0.2 * fmt + 0.2 * lng
    score100 = round(100 * score01, 1)

    if score100 >= 70:
        verdict = "âœ”âœ” Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙÙ‡ÙÙ… Ø§Ù„Ù€Prompt Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯"
    elif score100 >= 50:
        verdict = "âš ï¸ Ø§Ù„ÙÙ‡Ù… Ù…ØªÙˆØ³Ø· â€“ ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†"
    else:
        verdict = "âŒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù… ÙŠÙÙ‡Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¨Ø´ÙƒÙ„ ÙƒØ§ÙÙ"

    details = {
        "ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„Ø© (0-100)": round(sim01 * 100, 1),
        "Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ (0-100)": round(fmt * 100, 1),
        "Ø«Ø±Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (0-100)": round(lng * 100, 1),
        "Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„ÙƒÙ„ÙŠØ© (0-100)": score100,
        "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„ØªÙ†Ø³ÙŠÙ‚": " | ".join(fmt_reasons) if fmt_reasons else "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…Ø· Ù…Ø·Ù„ÙˆØ¨",
    }
    return score100, verdict, details

def run_test(prompt, model_repo, max_new_tokens, temperature):
    response, ok, status = generate_response(prompt, model_repo, max_new_tokens, temperature)
    if not response:
        return "", status, {}, ""
    score, verdict, details = overall_evaluate(prompt, response)

    # Pretty details
    details_txt = "\n".join([f"- {k}: {v}" for k, v in details.items()])
    return response, verdict, details, details_txt

# ------------- UI -------------
with gr.Blocks(title="Ø§Ø®ØªØ¨Ø§Ø± ÙÙ‡Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ù†ØµÙˆØµ") as demo:
    gr.Markdown("""
# ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± ÙÙ‡Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ù†ØµÙˆØµ (Prompt Understanding Test)
Ø§ÙƒØªØ¨ Ø£ÙŠ Prompt ÙˆØ´Ø§Ù‡Ø¯ Ø±Ø¯Ù‘ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙÙ‡Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§.
> **Ù…Ù‡Ù…:** ÙŠØ¬Ø¨ Ø¶Ø¨Ø· Ø³Ø±Ù‘ `HF_TOKEN` Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù€Space ÙˆØ§Ø®ØªÙŠØ§Ø± Ù…ÙˆØ¯ÙŠÙ„ Ù…ÙØªÙˆØ­.
    """)

    with gr.Row():
        model_repo = gr.Dropdown(
            choices=[
                "HuggingFaceH4/zephyr-7b-beta",
                "google/gemma-2-2b-it",
                "mistralai/Mistral-7B-Instruct-v0.2",
                "tiiuae/falcon-7b-instruct",
            ],
            value=DEFAULT_MODEL,
            label="HF Model Repo",
            info="ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ù† Ù‡Ù†Ø§",
        )
    prompt = gr.Textbox(lines=6, label="Ø§ÙƒØªØ¨ Ø§Ù„Ù€Prompt Ù‡Ù†Ø§")
    with gr.Row():
        max_new_tokens = gr.Slider(32, 512, value=256, step=1, label="Max New Tokens")
        temperature = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label="Temperature")

    btn = gr.Button("Ø§Ø®ØªØ¨Ø± Ø§Ù„Ø¢Ù† âœ¨")

    with gr.Row():
        response = gr.Textbox(label="Ø±Ø¯Ù‘ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬", lines=10)
    verdict = gr.Label(label="ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙÙ‡Ù…")
    details_dict = gr.JSON(label="ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª", value={})
    details_txt = gr.Textbox(label="ØªÙØ§ØµÙŠÙ„ Ù†ØµÙŠØ©", lines=6)

    btn.click(
        fn=run_test,
        inputs=[prompt, model_repo, max_new_tokens, temperature],
        outputs=[response, verdict, details_dict, details_txt]
    )

if __name__ == "__main__":
    demo.launch()